{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-21T05:09:47.590093400Z",
     "start_time": "2023-07-21T05:09:41.609752800Z"
    }
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf, sum, col, array_construct, month, year, call_udf, lit\n",
    "from snowflake.snowpark.types import Variant\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from snowflake.ml.modeling.linear_model import LinearRegression\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "\n",
    "# Misc\n",
    "import json\n",
    "import logging\n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "connection_parameters = json.load(open(\"C:\\\\Users\\\\argupta\\\\Snowflake\\\\Snowpark\\\\auth.json\"))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T05:09:49.681390900Z",
     "start_time": "2023-07-21T05:09:47.590093400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "session.use_database('demo_db')\n",
    "session.use_schema('public')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T05:09:50.345203800Z",
     "start_time": "2023-07-21T05:09:49.684326700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "---------------------------------------------------------------------\n",
      "|516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|467736           |474679          |469856   |469784   |2995042.21  |\n",
      "|518044           |523408          |523688   |519430   |3310662.6   |\n",
      "|521339           |521528          |519625   |521698   |3314107.1   |\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_spend_revenue_per_month = session.table('spend_and_revenue_per_month')\n",
    "\n",
    "# Delete rows with missing values\n",
    "df_spend_revenue_per_month = df_spend_revenue_per_month.dropna()\n",
    "\n",
    "# Exclude columns we don't need for modeling\n",
    "df_spend_revenue_per_month = df_spend_revenue_per_month.drop(['YEAR', 'MONTH'])\n",
    "\n",
    "# Save features into Snowflake table call MARKETING_BUDGET_FEATURES\n",
    "df_spend_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGET_FEATURES')\n",
    "df_spend_revenue_per_month.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T05:09:53.063348400Z",
     "start_time": "2023-07-21T05:09:50.345203800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on Train : 0.9579410972127744\n",
      "R2 score on test : 0.8402022470039145\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "\n",
    "CROSS_VALIDATION_FOLDS = 10\n",
    "POLYNOMIAL_FEATURE_DEGREE = 2\n",
    "\n",
    "# Create train and test dataframes\n",
    "train_df, test_df = session.table(\"MARKETING_BUDGET_FEATURES\").random_split(weights=[0.8, 0.2], seed=0)\n",
    "\n",
    "# Preprocess the numeric columns\n",
    "# We apply PolynomialFeatures and StandardScalar preprocessing steps to the numeric columns\n",
    "numeric_features = ['SEARCH_ENGINE', 'SOCIAL_MEDIA', 'VIDEO', 'EMAIL']\n",
    "numeric_transformer = Pipeline(steps=[('poly', PolynomialFeatures(degree = POLYNOMIAL_FEATURE_DEGREE)), ('scalar', StandardScaler())])\n",
    "\n",
    "# Combine the preprocessed step together using the column transformer module\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The next step is to integrate the features we just preprocessed with ML algo\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearRegression())])\n",
    "parameters = {}\n",
    "\n",
    "# Use GridSearch to find the best fitting model based on number_of_folds\n",
    "model = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = parameters,\n",
    "    cv = CROSS_VALIDATION_FOLDS,\n",
    "    label_cols = [\"REVENUE\"],\n",
    "    output_cols = [\"PREDICTED_REVENUE\"],\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "# Fit and Score\n",
    "model.fit(train_df)\n",
    "train_r2_score = model.score(train_df)\n",
    "test_r2_score = model.score(test_df)\n",
    "\n",
    "# R2 score on train and test datasets\n",
    "print(f\"R2 score on Train : {train_r2_score}\")\n",
    "print(f\"R2 score on test : {test_r2_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T08:01:55.628639200Z",
     "start_time": "2023-07-21T08:00:29.792699400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['SEARCH_ENGINE', 'SOCIAL_MEDIA', 'VIDEO', 'EMAIL']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T07:46:02.402430400Z",
     "start_time": "2023-07-21T07:46:02.386442400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[PutResult(source='model.joblib', target='model.joblib.gz', source_size=5348, target_size=2448, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained Model to Snowflake Stage\n",
    "\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# Extract SKLearn object\n",
    "sk_model = model.to_sklearn()\n",
    "\n",
    "model_output_dir = '/tmp'\n",
    "model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "dump(sk_model, model_file)\n",
    "session.file.put(model_file, \"@camp_models\", overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T08:07:38.711448300Z",
     "start_time": "2023-07-21T08:07:37.228614400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Create Scalar UDF for inference\n",
    "\"\"\"\n",
    "Now to deploy this model for inference, let's create and register a Snowpark Python UDF and add the trained model as a dependency. Once registered, getting new predictions is as simple as calling the function by passing in data.\n",
    "\"\"\"\n",
    "\n",
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "# Add trained model and Python packages from snowflake anaconda channel available on the server side as UDF dependencies\n",
    "session.add_import('@camp_models/model.joblib.gz')\n",
    "session.add_packages('pandas', 'joblib', 'scikit-learn==1.1.1')\n",
    "\n",
    "@udf(name='predict_roi', session=session, replace=True, is_permanent=True, stage_location='camp_udfs')\n",
    "def predict_roi(budget_allocations: list) -> float:\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    from joblib import load\n",
    "    import sklearn\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    model_file = import_dir + 'model.joblib.gz'\n",
    "    model = load(model_file)\n",
    "\n",
    "    features = ['SEARCH_ENGINE', 'SOCIAL_MEDIA', 'VIDEO', 'EMAIL']\n",
    "    df = pd.DataFrame([budget_allocations], columns=features)\n",
    "    roi = abs(model.predict(df)[0])\n",
    "    return roi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T08:21:20.937436300Z",
     "start_time": "2023-07-21T08:21:06.518496400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_ROI\"     |\n",
      "-----------------------------------------------------------------------------\n",
      "|500000           |500000          |500000   |500000   |3182207.8960703956  |\n",
      "|250000           |250000          |200000   |450000   |25414662.867387168  |\n",
      "|8500             |9500            |2000     |500      |2283241.0106569445  |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Scalar UDF for inference on new data\n",
    "\n",
    "test_df_2 = session.create_dataframe([[250000,250000,200000,450000],[500000,500000,500000,500000],[8500,9500,2000,500]],\n",
    "                                     schema=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL'])\n",
    "test_df_2.select(\n",
    "    'SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL',\n",
    "    call_udf(\"predict_roi\",\n",
    "             array_construct(col(\"SEARCH_ENGINE\"), col(\"SOCIAL_MEDIA\"), col(\"VIDEO\"), col(\"EMAIL\"))).as_(\"PREDICTED_ROI\")\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T08:30:23.502686200Z",
     "start_time": "2023-07-21T08:30:21.297817100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Create Vectorized User-Defined Function (UDF) using Batch API for inference\n",
    "\"\"\"\n",
    "Here we will leverage the Python UDF Batch API to create a vectorized UDF which takes a Pandas Dataframe as input. This means that each call to the UDF receives a set/batch of rows compared to a Scalar UDF which gets one row as input.\n",
    "\n",
    "First we will create a helper function load_model() that uses cachetools to make sure we only load the model once followed by batch_predict_roi() function that does the inference.\n",
    "\"\"\"\n",
    "\n",
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "import cachetools\n",
    "from snowflake.snowpark.types import PandasSeries, PandasDataFrame\n",
    "\n",
    "session.add_import('@camp_models/model.joblib.gz')\n",
    "session.add_packages('pandas','joblib','scikit-learn','cachetools')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "@udf(name='batch_predict_roi', session=session, replace=True, is_permanent=True, stage_location='@camp_udfs')\n",
    "def batch_predict_roi(budget_allocation_df: PandasDataFrame[int, int, int, int]) -> PandasSeries[float]:\n",
    "    import sklearn\n",
    "    budget_allocation_df.columns=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "    model = load_model('model.joblib.gz')\n",
    "    return abs(model.predict(budget_allocation_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T10:13:14.836895100Z",
     "start_time": "2023-07-21T10:13:07.815565600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_ROI\"     |\n",
      "-----------------------------------------------------------------------------\n",
      "|250000           |250000          |200000   |450000   |25414662.867387168  |\n",
      "|500000           |500000          |500000   |500000   |3182207.8960703956  |\n",
      "|8500             |9500            |2000     |500      |2283241.0106569445  |\n",
      "-----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Vectorized User-Defined Function (UDF) using Batch API for inference on new data points\n",
    "\n",
    "test_df_2.select(\n",
    "    'SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL',\n",
    "    call_udf(\"batch_predict_roi\",\n",
    "             col(\"SEARCH_ENGINE\"), col(\"SOCIAL_MEDIA\"), col(\"VIDEO\"), col(\"EMAIL\")).as_(\"PREDICTED_ROI\")\n",
    ").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T10:15:25.268661300Z",
     "start_time": "2023-07-21T10:15:23.207032700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
